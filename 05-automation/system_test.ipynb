{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Modules Functionality Workbook\n",
    "\n",
    "This notebook demonstrates all steps of the workflow: fetching, transforming, training, inference, and simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check local directory is 05-deployment-and-automation\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow settings\n",
    "FETCH_REPO = True\n",
    "TRANSFORM_DATA = True\n",
    "TRAIN_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Fetching data from API / Loading from a local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_repo import DataRepository\n",
    "\n",
    "repo = DataRepository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FETCH_REPO:\n",
    "  # Fetch All 3 datasets for all dates from APIs\n",
    "  repo.fetch()\n",
    "  # save data to a local dir\n",
    "  repo.persist(data_dir='local_data/')\n",
    "else:\n",
    "  # OR Load from disk\n",
    "  repo.load(data_dir='local_data/')  \n",
    "\n",
    "  \n",
    "# DEBUG: Separate fetching of the datasets (YFinance-Tickers, YFinance-Indexes, FRED-Macro)\n",
    "# repo.fetch_tickers()\n",
    "# repo.fetch_indexes()\n",
    "# repo.fetch_macro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING YET - need to move all data transformations to Transform\n",
    "\n",
    "# # Test fetching data with a specific min_date\n",
    "# min_date = '2024-06-01'\n",
    "# print(f\"\\nFetching data from {min_date}:\")\n",
    "# repo.fetch(min_date=min_date)\n",
    "\n",
    "# repo.ticker_df.Ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.ticker_df.Date.agg({'min','max','count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.ticker_df.Ticker.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "repo.ticker_df.groupby('Date')['Open'].agg('count').plot()\n",
    "plt.title('How quicky tickers data appear in the dataframe?')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.indexes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.indexes_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.macro_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Transform all input fields (data_repo dfs) to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.transform import TransformData\n",
    "\n",
    "transformed =  TransformData(repo = repo)\n",
    "\n",
    "if TRANSFORM_DATA:\n",
    "  transformed.transform()\n",
    "  transformed.persist(data_dir='local_data/')\n",
    "else:\n",
    "  transformed.load(data_dir='local_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.transformed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.transformed_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 3 days of the data\n",
    "transformed.transformed_df.tail(3)['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Train the best model \n",
    "* Model : Random Forest(max_depth=17, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import TrainModel\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings (not recommended in production unless necessary)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trained = TrainModel(transformed=transformed)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "  trained.prepare_dataframe() # prepare dataframes\n",
    "  trained.train_random_forest() # train the model\n",
    "  trained.persist(data_dir='local_data/') # save the model to disk\n",
    "else:\n",
    "  trained.prepare_dataframe() # prepare dataframes (incl. for inference)\n",
    "  trained.load(data_dir='local_data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting df\n",
    "trained.df_full.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_name='pred_rf_best'\n",
    "trained.make_inference(pred_name=prediction_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained.df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['Adj Close','Ticker','Date',prediction_name, prediction_name+'_rank']\n",
    "trained.df_full[trained.df_full[f'{prediction_name}_rank']==1].sort_values(by=\"Date\").tail(10)[COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained.df_full[trained.df_full[f'{prediction_name}_rank']<=3].sort_values(by=\"Date\").tail(10)[COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST DATE\n",
    "tickers = trained.df_full[trained.df_full[f'{prediction_name}_rank']<=3].sort_values(by=\"Date\").tail(3)['Ticker'].to_list()\n",
    "tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when made predictions?\n",
    "from datetime import datetime  # Import the datetime module\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "print(f\"Current date and time: {current_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for one Ticker by date - is it the last day's jump?\n",
    "print(trained.df_full[trained.df_full['Ticker'].isin(tickers)].sort_values(by=\"Date\").tail(10)[COLUMNS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several things on the predictions to choose from:\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a reference to the DataFrame in `trained.df_full`\n",
    "df = trained.df_full\n",
    "\n",
    "# Ensure the `Date` column is in datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by=['Ticker', 'Date'])\n",
    "\n",
    "# List of tickers for reference\n",
    "# top 3 predictions defined previously\n",
    "\n",
    "# Function to calculate growth percentage for a given interval\n",
    "def calculate_growth(df, days):\n",
    "    df[f'Adj_Close_{days}d_growth'] = df.groupby('Ticker')['Adj Close'].pct_change(periods=days) * 100\n",
    "    return df\n",
    "\n",
    "# Calculate growths for 1d, 5d, 30d, 90d, 225d\n",
    "intervals = [1, 5, 30, 90, 225]\n",
    "for interval in intervals:\n",
    "    df = calculate_growth(df, interval)\n",
    "\n",
    "# 1. Growth Visualization: Grouped bar chart for each interval\n",
    "growth_df = df.melt(id_vars=['Date', 'Ticker'], \n",
    "                    value_vars=[f'Adj_Close_{interval}d_growth' for interval in intervals], \n",
    "                    var_name='Growth Interval', \n",
    "                    value_name='Growth %')\n",
    "growth_df['Growth Interval'] = growth_df['Growth Interval'].str.extract(r'(\\d+)').astype(int)  # Extract days as integer\n",
    "\n",
    "# Plot grouped bar chart\n",
    "fig = px.bar(growth_df.dropna(), x='Ticker', y='Growth %', color='Growth Interval',\n",
    "             barmode='group', title=\"Growth in Adj Close (%) over Different Time Intervals\",\n",
    "             category_orders={\"Growth Interval\": intervals})  # To ensure order\n",
    "fig.show()\n",
    "\n",
    "# # 2. Predictions Graph (Last Month) for each Ticker\n",
    "# # Filter for the last month's data\n",
    "# last_month = df['Date'].max() - pd.DateOffset(days=30)\n",
    "# last_month_df = df[df['Date'] >= last_month]\n",
    "\n",
    "# # Line chart for predictions over the last month\n",
    "# fig = px.line(last_month_df, x='Date', y='pred_rf_best', color='Ticker', \n",
    "#               title=\"Prediction Trends for the Last Month\",\n",
    "#               labels={'pred_rf_best': 'Prediction Value'})\n",
    "# fig.show()\n",
    "\n",
    "# # 3. 52-Week High, Low, and Current Price for Each Ticker\n",
    "# from datetime import timedelta\n",
    "\n",
    "# one_year_ago = df['Date'].max() - timedelta(days=365)\n",
    "# summary = {}\n",
    "\n",
    "# for ticker in tickers:\n",
    "#     ticker_df = df[(df['Ticker'] == ticker) & (df['Date'] >= one_year_ago)]\n",
    "#     current_price = df[df['Ticker'] == ticker].iloc[-1]['Adj Close']\n",
    "#     min_price_52w = ticker_df['Adj Close'].min()\n",
    "#     max_price_52w = ticker_df['Adj Close'].max()\n",
    "    \n",
    "#     summary[ticker] = {\n",
    "#         'Current Price': current_price,\n",
    "#         '52-Week High': max_price_52w,\n",
    "#         '52-Week Low': min_price_52w,\n",
    "#         'Position': f\"{round((current_price - min_price_52w) / (max_price_52w - min_price_52w) * 100, 2)}%\"\n",
    "#     }\n",
    "\n",
    "# print(\"52-Week Summary for Each Ticker:\")\n",
    "# for ticker, stats in summary.items():\n",
    "#     print(f\"\\nTicker: {ticker}\")\n",
    "#     print(f\"  Current Price: {stats['Current Price']}\")\n",
    "#     print(f\"  52-Week High: {stats['52-Week High']}\")\n",
    "#     print(f\"  52-Week Low: {stats['52-Week Low']}\")\n",
    "#     print(f\"  Position within 52-Week Range: {stats['Position']}\")\n",
    "\n",
    "# # 4. Volatility (Standard Deviation) for 1 week and overall\n",
    "# volatility = {}\n",
    "\n",
    "# for ticker in tickers:\n",
    "#     ticker_df = df[df['Ticker'] == ticker]\n",
    "#     # 1-Week Volatility\n",
    "#     one_week_df = ticker_df[ticker_df['Date'] >= ticker_df['Date'].max() - timedelta(days=7)]\n",
    "#     volatility[ticker] = {\n",
    "#         '1-Week Volatility': one_week_df['Adj Close'].std(),\n",
    "#         'Overall Volatility': ticker_df['Adj Close'].std()\n",
    "#     }\n",
    "\n",
    "# print(\"\\nVolatility Summary for Each Ticker:\")\n",
    "# for ticker, stats in volatility.items():\n",
    "#     print(f\"\\nTicker: {ticker}\")\n",
    "#     print(f\"  1-Week Volatility: {stats['1-Week Volatility']}\")\n",
    "#     print(f\"  Overall Volatility: {stats['Overall Volatility']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of predictions for one stock\n",
    "# print(trained.df_full[trained.df_full['Ticker']=='VZ'].sort_values(by=\"Date\").tail(10)[COLUMNS])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
