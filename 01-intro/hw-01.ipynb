{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86240dcd",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1: [Index] S&P 500 Stocks Added to the Index\n",
    "\n",
    "**Which year had the highest number of additions?**\n",
    "\n",
    "Using the list of S&P 500 companies from Wikipedia's [S&P 500 companies page](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies), download the data including the year each company was added to the index.\n",
    "* Create a DataFrame with company tickers, names, and the year they were added.\n",
    "* Extract the year from the addition date and calculate the number of stocks added each year.\n",
    "* Which year had the highest number of additions (1957 doesn't count, as it was the year when the S&P 500 index was founded)? Write down this year as your answer (the most recent one, if you have several records).\n",
    "\n",
    "*Context*: \n",
    "> \"Following the announcement, all four new entrants saw their stock prices rise in extended trading on Friday\" - recent examples of S&P 500 additions include DASH, WSM, EXE, TKO in 2025 ([Nasdaq article](https://www.nasdaq.com/articles/sp-500-reshuffle-dash-tko-expe-wsm-join-worth-buying)).\n",
    "\n",
    "*Additional*: How many current S&P 500 stocks have been in the index for more than 20 years? When stocks are added to the S&P 500, they usually experience a price bump as investors and index funds buy shares following the announcement.\n",
    "\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "#Data viz\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44476338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year with the most additions to S&P 500 (excluding 1957): 2017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "tables = pd.read_html(url)\n",
    "df = tables[0].copy()\n",
    "\n",
    "\n",
    "final_df= df[['Symbol','Security','Founded','Date added']].copy()\n",
    "\n",
    "# Convert 'Date added' to datetime, ignoring errors\n",
    "\n",
    "final_df['Date added'] = pd.to_datetime(final_df['Date added'], errors='coerce')\n",
    "\n",
    "# Drop NaT (missing values in Date added)\n",
    "final_df = final_df.dropna(subset=['Date added'])\n",
    "\n",
    "# Extract year from 'Date added'\n",
    "final_df['Year added'] = final_df['Date added'].dt.year\n",
    "\n",
    "# Count number of additions per year\n",
    "additions_per_year = final_df['Year added'].value_counts().sort_index()\n",
    "\n",
    "# Exclude 1957\n",
    "additions_filtered = additions_per_year[additions_per_year.index != 1957]\n",
    "\n",
    "# Find the year with the highest number of additions (most recent if tie)\n",
    "max_additions = additions_filtered.max()\n",
    "most_recent_max_year = additions_filtered[additions_filtered == max_additions].index.max()\n",
    "\n",
    "print(f\"Year with the most additions to S&P 500 (excluding 1957): {most_recent_max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466c71bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks in S&P 500 for more than 20 years: 427\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year\n",
    "current_year\n",
    "\n",
    "final_df['Founded'] = pd.to_numeric(final_df['Founded'], errors='coerce')\n",
    "\n",
    "final_df['Years_in_Index'] = current_year - final_df['Founded']\n",
    "\n",
    "over_20_years = final_df[final_df['Years_in_Index'] > 20]\n",
    "print(f\"Stocks in S&P 500 for more than 20 years: {len(over_20_years)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b1d81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fcb5867",
   "metadata": {},
   "source": [
    "### Question 2. [Macro] Indexes YTD (as of 1 May 2025)\n",
    "\n",
    "**How many indexes (out of 10) have better year-to-date returns than the US (S&P 500) as of May 1, 2025?**\n",
    "\n",
    "Using Yahoo Finance World Indices data, compare the year-to-date (YTD) performance (1 January-1 May 2025) of major stock market indexes for the following countries:\n",
    "* United States - S&P 500 (^GSPC)\n",
    "* China - Shanghai Composite (000001.SS)\n",
    "* Hong Kong - HANG SENG INDEX (^HSI)\t\n",
    "* Australia - S&P/ASX 200 (^AXJO)\n",
    "* India - Nifty 50 (^NSEI)\n",
    "* Canada - S&P/TSX Composite (^GSPTSE)\n",
    "* Germany - DAX (^GDAXI)\n",
    "* United Kingdom - FTSE 100 (^FTSE)\n",
    "* Japan - Nikkei 225 (^N225)\n",
    "* Mexico - IPC Mexico (^MXX)\n",
    "* Brazil - Ibovespa (^BVSP)\n",
    "\n",
    "*Hint*: use start_date='2025-01-01' and end_date='2025-05-01' when downloading daily data in yfinance\n",
    "\n",
    "Context: \n",
    "> [Global Valuations: Who's Cheap, Who's Not?](https://simplywall.st/article/beyond-the-us-global-markets-after-yet-another-tariff-update) article suggests \"Other regions may be growing faster than the US and you need to diversify.\"\n",
    "\n",
    "Reference: Yahoo Finance World Indices - https://finance.yahoo.com/world-indices/\n",
    "\n",
    "*Additional*: How many of these indexes have better returns than the S&P 500 over 3, 5, and 10 year periods? Do you see the same trend?\n",
    "Note: For simplicity, ignore currency conversion effects.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2f0618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period for indexes: 2025-01-01 to 2025-05-01\n"
     ]
    }
   ],
   "source": [
    "# end = date.today()\n",
    "# print(f'Year = {end.year}; month= {end.month}; day={end.day}')\n",
    "\n",
    "# start = date(year=end.year-10, month=end.month, day=end.day)\n",
    "# print(f'Period for indexes: {start} to {end} ')\n",
    "\n",
    "start = date(year=2025, month=1, day=1)\n",
    "\n",
    "end = date(year=2025, month=5, day=1)\n",
    "print(f'Period for indexes: {start} to {end}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354b4def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_obj = yf.Ticker(\"^GSPC\")\n",
    "\n",
    "sandp500_daily = ticker_obj.history(start = start,end =end,interval = \"1d\")\n",
    "sandp500_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b225b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "class GetTickerData:\n",
    "    def __init__(self, tickers):\n",
    "        # Accept either a string (single ticker) or a list of tickers\n",
    "        if isinstance(tickers, str):\n",
    "            self.tickers = [tickers]\n",
    "        else:\n",
    "            self.tickers = tickers\n",
    "        self.data = {}\n",
    "\n",
    "    def get_data(self, start, end, combine=False):\n",
    "        \"\"\"\n",
    "        Download historical data for all tickers between start and end dates.\n",
    "\n",
    "        Parameters:\n",
    "        - start (str): Start date (e.g., '2023-01-01')\n",
    "        - end (str): End date (e.g., '2023-12-31')\n",
    "        - combine (bool): Whether to return a single combined DataFrame\n",
    "\n",
    "        Returns:\n",
    "        - dict: {ticker: DataFrame} if combine=False\n",
    "        - DataFrame: multi-index DataFrame if combine=True\n",
    "        \"\"\"\n",
    "        for ticker in self.tickers:\n",
    "            data = yf.Ticker(ticker).history(start=start, end=end, interval=\"1d\")\n",
    "            self.data[ticker] = data\n",
    "\n",
    "        if combine:\n",
    "            combined_df = pd.concat(self.data.values(), keys=self.data.keys(), names=[\"Ticker\", \"Date\"])\n",
    "            combined_df = combined_df.reset_index()\n",
    "            return combined_df\n",
    "        return self.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2f977b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-02 00:00:00-05:00</td>\n",
       "      <td>5903.259766</td>\n",
       "      <td>5935.089844</td>\n",
       "      <td>5829.529785</td>\n",
       "      <td>5868.549805</td>\n",
       "      <td>3621680000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-03 00:00:00-05:00</td>\n",
       "      <td>5891.069824</td>\n",
       "      <td>5949.339844</td>\n",
       "      <td>5888.660156</td>\n",
       "      <td>5942.470215</td>\n",
       "      <td>3667340000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-06 00:00:00-05:00</td>\n",
       "      <td>5982.810059</td>\n",
       "      <td>6021.040039</td>\n",
       "      <td>5960.009766</td>\n",
       "      <td>5975.379883</td>\n",
       "      <td>4940120000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-07 00:00:00-05:00</td>\n",
       "      <td>5993.259766</td>\n",
       "      <td>6000.680176</td>\n",
       "      <td>5890.680176</td>\n",
       "      <td>5909.029785</td>\n",
       "      <td>4517330000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-08 00:00:00-05:00</td>\n",
       "      <td>5910.660156</td>\n",
       "      <td>5927.890137</td>\n",
       "      <td>5874.779785</td>\n",
       "      <td>5918.250000</td>\n",
       "      <td>4441740000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                       Date         Open         High          Low  \\\n",
       "0  ^GSPC  2025-01-02 00:00:00-05:00  5903.259766  5935.089844  5829.529785   \n",
       "1  ^GSPC  2025-01-03 00:00:00-05:00  5891.069824  5949.339844  5888.660156   \n",
       "2  ^GSPC  2025-01-06 00:00:00-05:00  5982.810059  6021.040039  5960.009766   \n",
       "3  ^GSPC  2025-01-07 00:00:00-05:00  5993.259766  6000.680176  5890.680176   \n",
       "4  ^GSPC  2025-01-08 00:00:00-05:00  5910.660156  5927.890137  5874.779785   \n",
       "\n",
       "         Close      Volume  Dividends  Stock Splits  \n",
       "0  5868.549805  3621680000        0.0           0.0  \n",
       "1  5942.470215  3667340000        0.0           0.0  \n",
       "2  5975.379883  4940120000        0.0           0.0  \n",
       "3  5909.029785  4517330000        0.0           0.0  \n",
       "4  5918.250000  4441740000        0.0           0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [\"^GSPC\",\"000001.SS\",\"^HSI\",\"^AXJO\",\"^NSEI\",\"^GSPTSE\",\"^GDAXI\",\"^FTSE\",\"^N225\",\"^MXX\",\"^BVSP\"]\n",
    "data_fetcher = GetTickerData(tickers)\n",
    "all_data = data_fetcher.get_data(start=start, end=end, combine=True)\n",
    "\n",
    "all_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7413e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_growth_metrics(df):\n",
    "    \"\"\"\n",
    "    Adds Day-over-Day (DoD), Quarter-over-Quarter (QoQ), Year-over-Year (YoY), and Year-to-Date (YTD) growth\n",
    "    using the 'Date' column for year reference.\n",
    "    \n",
    "    Assumes 'Date' column exists and is datetime type.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure 'Date' column is datetime\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "\n",
    "    # Sort by Date in case it's not sorted\n",
    "    df = df.sort_values('Date')\n",
    "\n",
    "    # Day-over-Day % change\n",
    "    df['DoD_Growth'] = df['Close'].pct_change()\n",
    "\n",
    "    # QoQ and YoY growth by trading days\n",
    "    df['QoQ_Growth'] = df['Close'].pct_change(periods=63)\n",
    "    df['YoY_Growth'] = df['Close'].pct_change(periods=252)\n",
    "\n",
    "    # YTD growth based on first trading day of the year\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['YTD_Base'] = df.groupby('Year')['Close'].transform('first')\n",
    "    df['YTD_Growth'] = (df['Close'] / df['YTD_Base']) - 1\n",
    "\n",
    "    # Clean up\n",
    "    df.drop(columns=['YTD_Base'], inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6de0c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>DoD_Growth</th>\n",
       "      <th>QoQ_Growth</th>\n",
       "      <th>YoY_Growth</th>\n",
       "      <th>Year</th>\n",
       "      <th>YTD_Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^NSEI</td>\n",
       "      <td>2024-12-31 18:30:00+00:00</td>\n",
       "      <td>23637.650391</td>\n",
       "      <td>23822.800781</td>\n",
       "      <td>23562.800781</td>\n",
       "      <td>23742.900391</td>\n",
       "      <td>154900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^AXJO</td>\n",
       "      <td>2025-01-01 13:00:00+00:00</td>\n",
       "      <td>8159.100098</td>\n",
       "      <td>8204.200195</td>\n",
       "      <td>8146.600098</td>\n",
       "      <td>8201.200195</td>\n",
       "      <td>304400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.654583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^HSI</td>\n",
       "      <td>2025-01-01 16:00:00+00:00</td>\n",
       "      <td>19932.800781</td>\n",
       "      <td>19932.800781</td>\n",
       "      <td>19542.980469</td>\n",
       "      <td>19623.320312</td>\n",
       "      <td>4033400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.392738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>1.392738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SS</td>\n",
       "      <td>2025-01-01 16:00:00+00:00</td>\n",
       "      <td>3347.938965</td>\n",
       "      <td>3351.721924</td>\n",
       "      <td>3242.086914</td>\n",
       "      <td>3262.561035</td>\n",
       "      <td>561400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.833741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>-0.602185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^NSEI</td>\n",
       "      <td>2025-01-01 18:30:00+00:00</td>\n",
       "      <td>23783.000000</td>\n",
       "      <td>24226.699219</td>\n",
       "      <td>23751.550781</td>\n",
       "      <td>24188.650391</td>\n",
       "      <td>283200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.414007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025</td>\n",
       "      <td>1.949404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker                      Date          Open          High  \\\n",
       "0      ^NSEI 2024-12-31 18:30:00+00:00  23637.650391  23822.800781   \n",
       "1      ^AXJO 2025-01-01 13:00:00+00:00   8159.100098   8204.200195   \n",
       "2       ^HSI 2025-01-01 16:00:00+00:00  19932.800781  19932.800781   \n",
       "3  000001.SS 2025-01-01 16:00:00+00:00   3347.938965   3351.721924   \n",
       "4      ^NSEI 2025-01-01 18:30:00+00:00  23783.000000  24226.699219   \n",
       "\n",
       "            Low         Close      Volume  Dividends  Stock Splits  \\\n",
       "0  23562.800781  23742.900391      154900        0.0           0.0   \n",
       "1   8146.600098   8201.200195      304400        0.0           0.0   \n",
       "2  19542.980469  19623.320312  4033400000        0.0           0.0   \n",
       "3   3242.086914   3262.561035      561400        0.0           0.0   \n",
       "4  23751.550781  24188.650391      283200        0.0           0.0   \n",
       "\n",
       "   DoD_Growth  QoQ_Growth  YoY_Growth  Year  YTD_Growth  \n",
       "0         NaN         NaN         NaN  2024    0.000000  \n",
       "1   -0.654583         NaN         NaN  2025    0.000000  \n",
       "2    1.392738         NaN         NaN  2025    1.392738  \n",
       "3   -0.833741         NaN         NaN  2025   -0.602185  \n",
       "4    6.414007         NaN         NaN  2025    1.949404  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_growth = add_growth_metrics(all_data)\n",
    "final_df_growth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc63a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>YTD_Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>^BVSP</td>\n",
       "      <td>2025-04-30 03:00:00+00:00</td>\n",
       "      <td>15.469175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>^MXX</td>\n",
       "      <td>2025-04-30 06:00:00+00:00</td>\n",
       "      <td>5.859884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>^N225</td>\n",
       "      <td>2025-04-29 15:00:00+00:00</td>\n",
       "      <td>3.395135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>^GSPTSE</td>\n",
       "      <td>2025-04-30 04:00:00+00:00</td>\n",
       "      <td>2.029032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>^NSEI</td>\n",
       "      <td>2025-04-29 18:30:00+00:00</td>\n",
       "      <td>1.967151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>^GDAXI</td>\n",
       "      <td>2025-04-29 22:00:00+00:00</td>\n",
       "      <td>1.743133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>^HSI</td>\n",
       "      <td>2025-04-29 16:00:00+00:00</td>\n",
       "      <td>1.697094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>^FTSE</td>\n",
       "      <td>2025-04-29 23:00:00+00:00</td>\n",
       "      <td>0.035812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>^AXJO</td>\n",
       "      <td>2025-04-29 14:00:00+00:00</td>\n",
       "      <td>-0.009145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-04-30 04:00:00+00:00</td>\n",
       "      <td>-0.320946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>000001.SS</td>\n",
       "      <td>2025-04-29 16:00:00+00:00</td>\n",
       "      <td>-0.600177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Ticker                      Date  YTD_Growth\n",
       "884      ^BVSP 2025-04-30 03:00:00+00:00   15.469175\n",
       "887       ^MXX 2025-04-30 06:00:00+00:00    5.859884\n",
       "878      ^N225 2025-04-29 15:00:00+00:00    3.395135\n",
       "886    ^GSPTSE 2025-04-30 04:00:00+00:00    2.029032\n",
       "881      ^NSEI 2025-04-29 18:30:00+00:00    1.967151\n",
       "882     ^GDAXI 2025-04-29 22:00:00+00:00    1.743133\n",
       "880       ^HSI 2025-04-29 16:00:00+00:00    1.697094\n",
       "883      ^FTSE 2025-04-29 23:00:00+00:00    0.035812\n",
       "877      ^AXJO 2025-04-29 14:00:00+00:00   -0.009145\n",
       "885      ^GSPC 2025-04-30 04:00:00+00:00   -0.320946\n",
       "879  000001.SS 2025-04-29 16:00:00+00:00   -0.600177"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_ytd = (\n",
    "    final_df_growth.sort_values('Date')                # ensure dates are sorted\n",
    "    .groupby('Ticker', as_index=False)                 # group by Ticker\n",
    "    .tail(1)                                            # get the latest row per Ticker\n",
    "    .sort_values('YTD_Growth', ascending=False)        # sort by YTD growth\n",
    ")\n",
    "\n",
    "latest_ytd[['Ticker', 'Date', 'YTD_Growth']]  # show top 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252bae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticker\n",
       "000001.SS    78\n",
       "^AXJO        81\n",
       "^BVSP        81\n",
       "^FTSE        83\n",
       "^GDAXI       83\n",
       "^GSPC        81\n",
       "^GSPTSE      83\n",
       "^HSI         79\n",
       "^MXX         81\n",
       "^N225        78\n",
       "^NSEI        80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_growth.groupby('Ticker').size()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5362d8f3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a ticker (example: 'AAPL')\n",
    "ticker = '^HSI'  # Replace with the desired ticker\n",
    "\n",
    "# Filter the data for that ticker\n",
    "df_ticker = final_df_growth[final_df_growth['Ticker'] == ticker].copy()\n",
    "\n",
    "# Ensure 'Date' is datetime and set as index for time series plotting\n",
    "df_ticker['Date'] = pd.to_datetime(df_ticker['Date'])\n",
    "df_ticker = df_ticker.sort_values('Date').set_index('Date')\n",
    "\n",
    "# Simple line plot using pandas\n",
    "df_ticker['DoD_Growth'].plot.line()\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('DoD Growth')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec181df",
   "metadata": {},
   "source": [
    "### Question 3. [Index] S&P 500 Market Corrections Analysis\n",
    "\n",
    "\n",
    "**Calculate the median duration (in days) of significant market corrections in the S&P 500 index.**\n",
    "\n",
    "For this task, define a correction as an event when a stock index goes down by **more than 5%** from the closest all-time high maximum.\n",
    "\n",
    "Steps:\n",
    "1. Download S&P 500 historical data (1950-present) using yfinance\n",
    "2. Identify all-time high points (where price exceeds all previous prices)\n",
    "3. For each pair of consecutive all-time highs, find the minimum price in between\n",
    "4. Calculate drawdown percentages: (high - low) / high Ã— 100\n",
    "5. Filter for corrections with at least 5% drawdown\n",
    "6. Calculate the duration in days for each correction period\n",
    "7. Determine the 25th, 50th (median), and 75th percentiles for correction durations\n",
    "\n",
    "*Context:* \n",
    "> * Investors often wonder about the typical length of market corrections when deciding \"when to buy the dip\" ([Reddit discussion](https://www.reddit.com/r/investing/comments/1jrqnte/when_are_you_buying_the_dip/?rdt=64135)).\n",
    "\n",
    "> * [A Wealth of Common Sense - How Often Should You Expect a Stock Market Correction?](https://awealthofcommonsense.com/2022/01/how-often-should-you-expect-a-stock-market-correction/)\n",
    "\n",
    "*Hint (use this data to compare with your results)*: Here is the list of top 10 largest corrections by drawdown:\n",
    "* 2007-10-09 to 2009-03-09: 56.8% drawdown over 517 days\n",
    "* 2000-03-24 to 2002-10-09: 49.1% drawdown over 929 days\n",
    "* 1973-01-11 to 1974-10-03: 48.2% drawdown over 630 days\n",
    "* 1968-11-29 to 1970-05-26: 36.1% drawdown over 543 days\n",
    "* 2020-02-19 to 2020-03-23: 33.9% drawdown over 33 days\n",
    "* 1987-08-25 to 1987-12-04: 33.5% drawdown over 101 days\n",
    "* 1961-12-12 to 1962-06-26: 28.0% drawdown over 196 days\n",
    "* 1980-11-28 to 1982-08-12: 27.1% drawdown over 622 days\n",
    "* 2022-01-03 to 2022-10-12: 25.4% drawdown over 282 days\n",
    "* 1966-02-09 to 1966-10-07: 22.2% drawdown over 240 days\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61867f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18952, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1950-01-03 00:00:00-05:00</td>\n",
       "      <td>16.66</td>\n",
       "      <td>16.66</td>\n",
       "      <td>16.66</td>\n",
       "      <td>16.66</td>\n",
       "      <td>1260000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1950-01-04 00:00:00-05:00</td>\n",
       "      <td>16.85</td>\n",
       "      <td>16.85</td>\n",
       "      <td>16.85</td>\n",
       "      <td>16.85</td>\n",
       "      <td>1890000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1950-01-05 00:00:00-05:00</td>\n",
       "      <td>16.93</td>\n",
       "      <td>16.93</td>\n",
       "      <td>16.93</td>\n",
       "      <td>16.93</td>\n",
       "      <td>2550000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1950-01-06 00:00:00-05:00</td>\n",
       "      <td>16.98</td>\n",
       "      <td>16.98</td>\n",
       "      <td>16.98</td>\n",
       "      <td>16.98</td>\n",
       "      <td>2010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>1950-01-09 00:00:00-05:00</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.08</td>\n",
       "      <td>17.08</td>\n",
       "      <td>2520000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                      Date   Open   High    Low  Close   Volume  \\\n",
       "0  ^GSPC 1950-01-03 00:00:00-05:00  16.66  16.66  16.66  16.66  1260000   \n",
       "1  ^GSPC 1950-01-04 00:00:00-05:00  16.85  16.85  16.85  16.85  1890000   \n",
       "2  ^GSPC 1950-01-05 00:00:00-05:00  16.93  16.93  16.93  16.93  2550000   \n",
       "3  ^GSPC 1950-01-06 00:00:00-05:00  16.98  16.98  16.98  16.98  2010000   \n",
       "4  ^GSPC 1950-01-09 00:00:00-05:00  17.08  17.08  17.08  17.08  2520000   \n",
       "\n",
       "   Dividends  Stock Splits  \n",
       "0        0.0           0.0  \n",
       "1        0.0           0.0  \n",
       "2        0.0           0.0  \n",
       "3        0.0           0.0  \n",
       "4        0.0           0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandp_ticker = \"^GSPC\"\n",
    "\n",
    "start = date(year=1950, month=1, day=1)\n",
    "end = date(year=2025, month=5, day=1)\n",
    "\n",
    "data_fetcher = GetTickerData(sandp_ticker)\n",
    "sandpdata = data_fetcher.get_data(start=start, end=end, combine=True)\n",
    "\n",
    "print(sandpdata.shape)\n",
    "sandpdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a217e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sandpdata['AllTimeHigh'] = sandpdata['Close'].cummax()\n",
    "all_time_highs = sandpdata[sandpdata['Close'] == sandpdata['AllTimeHigh']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02db64e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>AllTimeHigh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18852</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2024-12-04 00:00:00-05:00</td>\n",
       "      <td>6069.390137</td>\n",
       "      <td>6089.839844</td>\n",
       "      <td>6061.060059</td>\n",
       "      <td>6086.490234</td>\n",
       "      <td>4003390000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6086.490234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18854</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2024-12-06 00:00:00-05:00</td>\n",
       "      <td>6081.379883</td>\n",
       "      <td>6099.970215</td>\n",
       "      <td>6079.979980</td>\n",
       "      <td>6090.270020</td>\n",
       "      <td>3924830000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6090.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18884</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-01-23 00:00:00-05:00</td>\n",
       "      <td>6076.319824</td>\n",
       "      <td>6118.729980</td>\n",
       "      <td>6074.669922</td>\n",
       "      <td>6118.709961</td>\n",
       "      <td>4432250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6118.709961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18901</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-02-18 00:00:00-05:00</td>\n",
       "      <td>6121.600098</td>\n",
       "      <td>6129.629883</td>\n",
       "      <td>6099.509766</td>\n",
       "      <td>6129.580078</td>\n",
       "      <td>4684980000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6129.580078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18902</th>\n",
       "      <td>^GSPC</td>\n",
       "      <td>2025-02-19 00:00:00-05:00</td>\n",
       "      <td>6117.759766</td>\n",
       "      <td>6147.430176</td>\n",
       "      <td>6111.149902</td>\n",
       "      <td>6144.149902</td>\n",
       "      <td>4562330000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6144.149902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ticker                      Date         Open         High          Low  \\\n",
       "18852  ^GSPC 2024-12-04 00:00:00-05:00  6069.390137  6089.839844  6061.060059   \n",
       "18854  ^GSPC 2024-12-06 00:00:00-05:00  6081.379883  6099.970215  6079.979980   \n",
       "18884  ^GSPC 2025-01-23 00:00:00-05:00  6076.319824  6118.729980  6074.669922   \n",
       "18901  ^GSPC 2025-02-18 00:00:00-05:00  6121.600098  6129.629883  6099.509766   \n",
       "18902  ^GSPC 2025-02-19 00:00:00-05:00  6117.759766  6147.430176  6111.149902   \n",
       "\n",
       "             Close      Volume  Dividends  Stock Splits  AllTimeHigh  \n",
       "18852  6086.490234  4003390000        0.0           0.0  6086.490234  \n",
       "18854  6090.270020  3924830000        0.0           0.0  6090.270020  \n",
       "18884  6118.709961  4432250000        0.0           0.0  6118.709961  \n",
       "18901  6129.580078  4684980000        0.0           0.0  6129.580078  \n",
       "18902  6144.149902  4562330000        0.0           0.0  6144.149902  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_time_highs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b1b14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = []\n",
    "\n",
    "for i in range(1, len(all_time_highs)):\n",
    "    start = all_time_highs.iloc[i-1]\n",
    "    end = all_time_highs.iloc[i]\n",
    "    \n",
    "    # Subset between the two ATHs (excluding the endpoints)\n",
    "    mask = (sandpdata['Date'] > start['Date']) & (sandpdata['Date'] < end['Date'])\n",
    "    if sandpdata[mask].empty:\n",
    "        continue\n",
    "    interim_low = sandpdata[mask]['Close'].min()\n",
    "    interim_low_date = sandpdata[mask].loc[sandpdata[mask]['Close'].idxmin(), 'Date']\n",
    "    \n",
    "    drawdown = (start['Close'] - interim_low) / start['Close'] * 100\n",
    "    reco_duration = (end['Date'] - interim_low_date).days\n",
    "    correction_duration = (interim_low_date - start['Date']).days \n",
    "    full_cycle_duration = (end['Date'] - start['Date']).days\n",
    "\n",
    "\n",
    "    \n",
    "    if drawdown >= 5:\n",
    "        corrections.append({\n",
    "            'StartDate': start['Date'],\n",
    "            'InterimLowDate': interim_low_date,\n",
    "            'EndDate': end['Date'],\n",
    "            'StartClose': start['Close'],\n",
    "            'LowClose': interim_low,\n",
    "            'EndClose': end['Close'],\n",
    "            'DrawdownPercent': drawdown,\n",
    "            'RecoveryDuration': reco_duration,\n",
    "            'DrawdownDuration': correction_duration,\n",
    "            'FullCycleDuration': full_cycle_duration\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba4fc32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartDate</th>\n",
       "      <th>InterimLowDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>StartClose</th>\n",
       "      <th>LowClose</th>\n",
       "      <th>EndClose</th>\n",
       "      <th>DrawdownPercent</th>\n",
       "      <th>RecoveryDuration</th>\n",
       "      <th>DrawdownDuration</th>\n",
       "      <th>FullCycleDuration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2007-10-09 00:00:00-04:00</td>\n",
       "      <td>2009-03-09 00:00:00-04:00</td>\n",
       "      <td>2013-03-28 00:00:00-04:00</td>\n",
       "      <td>1565.150024</td>\n",
       "      <td>676.530029</td>\n",
       "      <td>1569.189941</td>\n",
       "      <td>56.775388</td>\n",
       "      <td>1480</td>\n",
       "      <td>517</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2000-03-24 00:00:00-05:00</td>\n",
       "      <td>2002-10-09 00:00:00-04:00</td>\n",
       "      <td>2007-05-30 00:00:00-04:00</td>\n",
       "      <td>1527.459961</td>\n",
       "      <td>776.760010</td>\n",
       "      <td>1530.229980</td>\n",
       "      <td>49.146948</td>\n",
       "      <td>1694</td>\n",
       "      <td>928</td>\n",
       "      <td>2622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1973-01-11 00:00:00-05:00</td>\n",
       "      <td>1974-10-03 00:00:00-04:00</td>\n",
       "      <td>1980-07-17 00:00:00-04:00</td>\n",
       "      <td>120.239998</td>\n",
       "      <td>62.279999</td>\n",
       "      <td>121.440002</td>\n",
       "      <td>48.203593</td>\n",
       "      <td>2114</td>\n",
       "      <td>629</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1968-11-29 00:00:00-05:00</td>\n",
       "      <td>1970-05-26 00:00:00-04:00</td>\n",
       "      <td>1972-03-06 00:00:00-05:00</td>\n",
       "      <td>108.370003</td>\n",
       "      <td>69.290001</td>\n",
       "      <td>108.769997</td>\n",
       "      <td>36.061641</td>\n",
       "      <td>650</td>\n",
       "      <td>542</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2020-02-19 00:00:00-05:00</td>\n",
       "      <td>2020-03-23 00:00:00-04:00</td>\n",
       "      <td>2020-08-18 00:00:00-04:00</td>\n",
       "      <td>3386.149902</td>\n",
       "      <td>2237.399902</td>\n",
       "      <td>3389.780029</td>\n",
       "      <td>33.924960</td>\n",
       "      <td>148</td>\n",
       "      <td>32</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1987-08-25 00:00:00-04:00</td>\n",
       "      <td>1987-12-04 00:00:00-05:00</td>\n",
       "      <td>1989-07-26 00:00:00-04:00</td>\n",
       "      <td>336.769989</td>\n",
       "      <td>223.919998</td>\n",
       "      <td>338.049988</td>\n",
       "      <td>33.509515</td>\n",
       "      <td>599</td>\n",
       "      <td>101</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1961-12-12 00:00:00-05:00</td>\n",
       "      <td>1962-06-26 00:00:00-04:00</td>\n",
       "      <td>1963-09-03 00:00:00-04:00</td>\n",
       "      <td>72.639999</td>\n",
       "      <td>52.320000</td>\n",
       "      <td>72.660004</td>\n",
       "      <td>27.973568</td>\n",
       "      <td>434</td>\n",
       "      <td>195</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1980-11-28 00:00:00-05:00</td>\n",
       "      <td>1982-08-12 00:00:00-04:00</td>\n",
       "      <td>1982-11-03 00:00:00-05:00</td>\n",
       "      <td>140.520004</td>\n",
       "      <td>102.419998</td>\n",
       "      <td>142.869995</td>\n",
       "      <td>27.113582</td>\n",
       "      <td>83</td>\n",
       "      <td>621</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022-01-03 00:00:00-05:00</td>\n",
       "      <td>2022-10-12 00:00:00-04:00</td>\n",
       "      <td>2024-01-19 00:00:00-05:00</td>\n",
       "      <td>4796.560059</td>\n",
       "      <td>3577.030029</td>\n",
       "      <td>4839.810059</td>\n",
       "      <td>25.425097</td>\n",
       "      <td>464</td>\n",
       "      <td>281</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1966-02-09 00:00:00-05:00</td>\n",
       "      <td>1966-10-07 00:00:00-04:00</td>\n",
       "      <td>1967-05-04 00:00:00-04:00</td>\n",
       "      <td>94.059998</td>\n",
       "      <td>73.199997</td>\n",
       "      <td>94.320000</td>\n",
       "      <td>22.177335</td>\n",
       "      <td>209</td>\n",
       "      <td>239</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   StartDate            InterimLowDate  \\\n",
       "56 2007-10-09 00:00:00-04:00 2009-03-09 00:00:00-04:00   \n",
       "54 2000-03-24 00:00:00-05:00 2002-10-09 00:00:00-04:00   \n",
       "24 1973-01-11 00:00:00-05:00 1974-10-03 00:00:00-04:00   \n",
       "22 1968-11-29 00:00:00-05:00 1970-05-26 00:00:00-04:00   \n",
       "65 2020-02-19 00:00:00-05:00 2020-03-23 00:00:00-04:00   \n",
       "35 1987-08-25 00:00:00-04:00 1987-12-04 00:00:00-05:00   \n",
       "15 1961-12-12 00:00:00-05:00 1962-06-26 00:00:00-04:00   \n",
       "27 1980-11-28 00:00:00-05:00 1982-08-12 00:00:00-04:00   \n",
       "68 2022-01-03 00:00:00-05:00 2022-10-12 00:00:00-04:00   \n",
       "18 1966-02-09 00:00:00-05:00 1966-10-07 00:00:00-04:00   \n",
       "\n",
       "                     EndDate   StartClose     LowClose     EndClose  \\\n",
       "56 2013-03-28 00:00:00-04:00  1565.150024   676.530029  1569.189941   \n",
       "54 2007-05-30 00:00:00-04:00  1527.459961   776.760010  1530.229980   \n",
       "24 1980-07-17 00:00:00-04:00   120.239998    62.279999   121.440002   \n",
       "22 1972-03-06 00:00:00-05:00   108.370003    69.290001   108.769997   \n",
       "65 2020-08-18 00:00:00-04:00  3386.149902  2237.399902  3389.780029   \n",
       "35 1989-07-26 00:00:00-04:00   336.769989   223.919998   338.049988   \n",
       "15 1963-09-03 00:00:00-04:00    72.639999    52.320000    72.660004   \n",
       "27 1982-11-03 00:00:00-05:00   140.520004   102.419998   142.869995   \n",
       "68 2024-01-19 00:00:00-05:00  4796.560059  3577.030029  4839.810059   \n",
       "18 1967-05-04 00:00:00-04:00    94.059998    73.199997    94.320000   \n",
       "\n",
       "    DrawdownPercent  RecoveryDuration  DrawdownDuration  FullCycleDuration  \n",
       "56        56.775388              1480               517               1997  \n",
       "54        49.146948              1694               928               2622  \n",
       "24        48.203593              2114               629               2743  \n",
       "22        36.061641               650               542               1193  \n",
       "65        33.924960               148                32                180  \n",
       "35        33.509515               599               101                701  \n",
       "15        27.973568               434               195                629  \n",
       "27        27.113582                83               621                705  \n",
       "68        25.425097               464               281                746  \n",
       "18        22.177335               209               239                448  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections_df = pd.DataFrame(corrections)\n",
    "corrections_df.sort_values(by='DrawdownPercent', ascending=False, inplace=True)\n",
    "\n",
    "corrections_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45b3c3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25    21.5\n",
       "0.50    39.0\n",
       "0.75    89.0\n",
       "Name: DrawdownDuration, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles = corrections_df['DrawdownDuration'].quantile([0.25, 0.5, 0.75])\n",
    "percentiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f02bd",
   "metadata": {},
   "source": [
    "### Question 4.  [Stocks] Earnings Surprise Analysis for Amazon (AMZN)\n",
    "\n",
    "\n",
    "**Calculate the median 2-day percentage change in stock prices following positive earnings surprises days.**\n",
    "\n",
    "Steps:\n",
    "1. Load earnings data from CSV ([ha1_Amazon.csv](ha1_Amazon.csv)) containing earnings dates, EPS estimates, and actual EPS\n",
    "2. Download complete historical price data using yfinance\n",
    "3. Calculate 2-day percentage changes for all historical dates: for each sequence of 3 consecutive trading days (Day 1, Day 2, Day 3), compute the return as Close_Day3 / Close_Day1 - 1. (Assume Day 2 may correspond to the earnings announcement.)\n",
    "4. Identify positive earnings surprises (where \"actual EPS > estimated EPS\" OR \"Surprise (%)>0\")\n",
    "5. Calculate 2-day percentage changes following positive earnings surprises\n",
    "6. Compare the median 2-day percentage change for positive surprises vs. all historical dates\n",
    "\n",
    "Context: Earnings announcements, especially when they exceed analyst expectations, can significantly impact stock prices in the short term.\n",
    "\n",
    "Reference: Yahoo Finance earnings calendar - https://finance.yahoo.com/calendar/earnings?symbol=AMZN\n",
    "\n",
    "*Additional*: Is there a correlation between the magnitude of the earnings surprise and the stock price reaction? Does the market react differently to earnings surprises during bull vs. bear markets?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deafe024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "      <th>Earnings Date</th>\n",
       "      <th>EPS Estimate</th>\n",
       "      <th>Reported EPS</th>\n",
       "      <th>Surprise (%)</th>\n",
       "      <th>PositiveSurprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>2026-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>2026-02-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.74</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>1998-07-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.34</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>1998-04-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.92</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>1998-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.41</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>1997-10-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.29</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>1997-07-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Symbol           Company Earnings Date  EPS Estimate  Reported EPS  \\\n",
       "0     AMZN    Amazon.com Inc    2026-04-29           NaN           NaN   \n",
       "1     AMZN    Amazon.com Inc    2026-02-04           NaN           NaN   \n",
       "2     AMZN    Amazon.com Inc    2025-10-29           NaN           NaN   \n",
       "3     AMZN    Amazon.com Inc    2025-07-30           NaN           NaN   \n",
       "4     AMZN  Amazon.com, Inc.    2025-05-01           NaN           NaN   \n",
       "..     ...               ...           ...           ...           ...   \n",
       "111   AMZN  Amazon.com, Inc.    1998-07-22           NaN           NaN   \n",
       "112   AMZN  Amazon.com, Inc.    1998-04-27           NaN           NaN   \n",
       "113   AMZN  Amazon.com, Inc.    1998-01-22           NaN           NaN   \n",
       "114   AMZN  Amazon.com, Inc.    1997-10-27           NaN           NaN   \n",
       "115   AMZN  Amazon.com, Inc.    1997-07-10           NaN           NaN   \n",
       "\n",
       "     Surprise (%)  PositiveSurprise  \n",
       "0             NaN             False  \n",
       "1             NaN             False  \n",
       "2             NaN             False  \n",
       "3             NaN             False  \n",
       "4           16.74              True  \n",
       "..            ...               ...  \n",
       "111          1.34              True  \n",
       "112         13.92              True  \n",
       "113         11.41              True  \n",
       "114         13.29              True  \n",
       "115         13.33              True  \n",
       "\n",
       "[116 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn_earnings = pd.read_csv('ha1_Amazon.csv', sep=';')\n",
    "\n",
    "# Drop rows where all values are NaN\n",
    "amzn_earnings = amzn_earnings.dropna(how='all')\n",
    "\n",
    "# Clean 'Earnings Date' column\n",
    "amzn_earnings['Earnings Date'] = amzn_earnings['Earnings Date'].str.extract(r'(.*)\\sat')[0]\n",
    "amzn_earnings['Earnings Date'] = pd.to_datetime(amzn_earnings['Earnings Date'], errors='coerce')\n",
    "\n",
    "# Convert numeric columns\n",
    "amzn_earnings['Surprise (%)'] = pd.to_numeric(amzn_earnings['Surprise (%)'], errors='coerce')\n",
    "amzn_earnings['Reported EPS'] = pd.to_numeric(amzn_earnings['Reported EPS'], errors='coerce')\n",
    "amzn_earnings['EPS Estimate'] = pd.to_numeric(amzn_earnings['EPS Estimate'], errors='coerce')\n",
    "\n",
    "# --- 2. Filter Positive Surprises ---\n",
    "amzn_earnings['PositiveSurprise'] = (\n",
    "    (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) |\n",
    "    (amzn_earnings['Surprise (%)'] > 0)\n",
    ")\n",
    "\n",
    "positive_surprises = amzn_earnings[\n",
    "    amzn_earnings['PositiveSurprise'] &\n",
    "    amzn_earnings['Earnings Date'].notnull() &\n",
    "    (amzn_earnings['Earnings Date'] < pd.Timestamp.today()) &\n",
    "    amzn_earnings['Reported EPS'].notna() &\n",
    "    amzn_earnings['EPS Estimate'].notna()\n",
    "]\n",
    "\n",
    "amzn_earnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0ef4d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Close_Day1</th>\n",
       "      <th>Close_Day3</th>\n",
       "      <th>2DayReturn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-05-15</td>\n",
       "      <td>0.121875</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>1443120000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>-0.127659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-05-16</td>\n",
       "      <td>0.098438</td>\n",
       "      <td>0.098958</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>294000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>-0.054211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-05-19</td>\n",
       "      <td>0.088021</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>122136000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>0.071354</td>\n",
       "      <td>-0.164639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-05-20</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>109344000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>0.069792</td>\n",
       "      <td>-0.146494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-05-21</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>0.082292</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.071354</td>\n",
       "      <td>377064000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071354</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.051097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7046</th>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>201.649994</td>\n",
       "      <td>206.619995</td>\n",
       "      <td>201.259995</td>\n",
       "      <td>206.160004</td>\n",
       "      <td>34314800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.160004</td>\n",
       "      <td>201.119995</td>\n",
       "      <td>-0.024447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>204.630005</td>\n",
       "      <td>205.589996</td>\n",
       "      <td>202.649994</td>\n",
       "      <td>204.070007</td>\n",
       "      <td>29470400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>204.070007</td>\n",
       "      <td>203.100006</td>\n",
       "      <td>-0.004753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>201.610001</td>\n",
       "      <td>203.460007</td>\n",
       "      <td>200.059998</td>\n",
       "      <td>201.119995</td>\n",
       "      <td>42460900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201.119995</td>\n",
       "      <td>200.990005</td>\n",
       "      <td>-0.000646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7049</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>201.380005</td>\n",
       "      <td>205.759995</td>\n",
       "      <td>200.160004</td>\n",
       "      <td>203.100006</td>\n",
       "      <td>38938900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.100006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>198.899994</td>\n",
       "      <td>202.369995</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>200.990005</td>\n",
       "      <td>33350500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.990005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7051 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close      Volume  \\\n",
       "0    1997-05-15    0.121875    0.125000    0.096354    0.097917  1443120000   \n",
       "1    1997-05-16    0.098438    0.098958    0.085417    0.086458   294000000   \n",
       "2    1997-05-19    0.088021    0.088542    0.081250    0.085417   122136000   \n",
       "3    1997-05-20    0.086458    0.087500    0.081771    0.081771   109344000   \n",
       "4    1997-05-21    0.081771    0.082292    0.068750    0.071354   377064000   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "7046 2025-05-19  201.649994  206.619995  201.259995  206.160004    34314800   \n",
       "7047 2025-05-20  204.630005  205.589996  202.649994  204.070007    29470400   \n",
       "7048 2025-05-21  201.610001  203.460007  200.059998  201.119995    42460900   \n",
       "7049 2025-05-22  201.380005  205.759995  200.160004  203.100006    38938900   \n",
       "7050 2025-05-23  198.899994  202.369995  197.850006  200.990005    33350500   \n",
       "\n",
       "      Dividends  Stock Splits  Close_Day1  Close_Day3  2DayReturn  \n",
       "0           0.0           0.0    0.097917    0.085417   -0.127659  \n",
       "1           0.0           0.0    0.086458    0.081771   -0.054211  \n",
       "2           0.0           0.0    0.085417    0.071354   -0.164639  \n",
       "3           0.0           0.0    0.081771    0.069792   -0.146494  \n",
       "4           0.0           0.0    0.071354    0.075000    0.051097  \n",
       "...         ...           ...         ...         ...         ...  \n",
       "7046        0.0           0.0  206.160004  201.119995   -0.024447  \n",
       "7047        0.0           0.0  204.070007  203.100006   -0.004753  \n",
       "7048        0.0           0.0  201.119995  200.990005   -0.000646  \n",
       "7049        0.0           0.0  203.100006         NaN         NaN  \n",
       "7050        0.0           0.0  200.990005         NaN         NaN  \n",
       "\n",
       "[7051 rows x 11 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 3. Load and Prepare Stock Price Data ---\n",
    "ticker_obj = yf.Ticker(\"AMZN\")\n",
    "amzn_price = ticker_obj.history(start='1997-01-01', interval=\"1d\")\n",
    "\n",
    "\n",
    "\n",
    "# Compute 2-day return: Close on Day 3 / Close on Day 1 - 1\n",
    "amzn_price['Close_Day1'] = amzn_price['Close']\n",
    "amzn_price['Close_Day3'] = amzn_price['Close'].shift(-2)\n",
    "amzn_price['2DayReturn'] = (amzn_price['Close_Day3'] / amzn_price['Close_Day1']) - 1\n",
    "\n",
    "# Reset index and remove timezone\n",
    "amzn_price = amzn_price.reset_index()\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "\n",
    "\n",
    "amzn_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "830b7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid positive surprises: 63\n",
      "Median 2-day return (all dates): 0.16 %\n",
      "Median 2-day return (positive surprises): -0.05 %\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Merge and Analyze ---\n",
    "merged = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price.sort_values('Date'),\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ")\n",
    "\n",
    "# Remove NaN returns\n",
    "pos_surprise_returns = merged['2DayReturn'].dropna()\n",
    "\n",
    "# Median return comparison\n",
    "median_all = amzn_price['2DayReturn'].median()\n",
    "median_surprise = pos_surprise_returns.median()\n",
    "\n",
    "# --- 5. Output Results ---\n",
    "print(\"Valid positive surprises:\", len(pos_surprise_returns))\n",
    "print(\"Median 2-day return (all dates):\", round(median_all * 100, 2), \"%\")\n",
    "print(\"Median 2-day return (positive surprises):\", round(median_surprise * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a4bc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_surprise_returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a92192a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(pos_surprise_returns, bins=20)\n",
    "plt.title(\"2-Day Returns After Positive EPS Surprises\")\n",
    "plt.xlabel(\"Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95ec7ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "left keys must be sorted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m amzn_price = amzn_price.sort_values(\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. For each earnings date, find the nearest trading day before & after:\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#    (a) previous trading day\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m prev  = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_asof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositive_surprises\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEarnings Date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamzn_price\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mClose\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEarnings Date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m.rename(columns={\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mPrevDate\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mClose_Before\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#    (b) next trading day\u001b[39;00m\n\u001b[32m     17\u001b[39m next_ = pd.merge_asof(\n\u001b[32m     18\u001b[39m     positive_surprises[[\u001b[33m'\u001b[39m\u001b[33mEarnings Date\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m     19\u001b[39m     amzn_price[[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     direction=\u001b[33m'\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m ).rename(columns={\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mNextDate\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mClose\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mClose_After\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/stock-market-analytics/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:708\u001b[39m, in \u001b[36mmerge_asof\u001b[39m\u001b[34m(left, right, on, left_on, right_on, left_index, right_index, by, left_by, right_by, suffixes, tolerance, allow_exact_matches, direction)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03mPerform a merge by key distance.\u001b[39;00m\n\u001b[32m    458\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    689\u001b[39m \u001b[33;03m4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    691\u001b[39m op = _AsOfMerge(\n\u001b[32m    692\u001b[39m     left,\n\u001b[32m    693\u001b[39m     right,\n\u001b[32m   (...)\u001b[39m\u001b[32m    706\u001b[39m     direction=direction,\n\u001b[32m    707\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/stock-market-analytics/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1926\u001b[39m, in \u001b[36m_OrderedMerge.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, copy: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> DataFrame:\n\u001b[32m-> \u001b[39m\u001b[32m1926\u001b[39m     join_index, left_indexer, right_indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1928\u001b[39m     left_join_indexer: npt.NDArray[np.intp] | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1929\u001b[39m     right_join_indexer: npt.NDArray[np.intp] | \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/stock-market-analytics/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1151\u001b[39m, in \u001b[36m_MergeOperation._get_join_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m     join_index, right_indexer, left_indexer = _left_join_on_index(\n\u001b[32m   1148\u001b[39m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m.right_join_keys, sort=\u001b[38;5;28mself\u001b[39m.sort\n\u001b[32m   1149\u001b[39m     )\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     (left_indexer, right_indexer) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index:\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.left) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/stock-market-analytics/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:2238\u001b[39m, in \u001b[36m_AsOfMerge._get_join_indexers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2235\u001b[39m         tolerance = tolerance._value\n\u001b[32m   2237\u001b[39m \u001b[38;5;66;03m# initial type conversion as needed\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m left_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_values_for_libjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2239\u001b[39m right_values = \u001b[38;5;28mself\u001b[39m._convert_values_for_libjoin(right_values, \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2241\u001b[39m \u001b[38;5;66;03m# a \"by\" parameter requires special handling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/stock-market-analytics/.venv/lib/python3.11/site-packages/pandas/core/reshape/merge.py:2182\u001b[39m, in \u001b[36m_AsOfMerge._convert_values_for_libjoin\u001b[39m\u001b[34m(self, values, side)\u001b[39m\n\u001b[32m   2180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m isna(values).any():\n\u001b[32m   2181\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMerge keys contain null values on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m side\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m keys must be sorted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrowExtensionArray):\n\u001b[32m   2185\u001b[39m     values = values._maybe_convert_datelike_array()\n",
      "\u001b[31mValueError\u001b[39m: left keys must be sorted"
     ]
    }
   ],
   "source": [
    "# 1. Ensure your price DataFrame is sorted and has a timezone-naÃ¯ve Date column\n",
    "amzn_price = ticker_obj.history(start='1997-01-01', interval='1d').reset_index()\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "amzn_price = amzn_price.sort_values('Date')\n",
    "\n",
    "# 2. For each earnings date, find the nearest trading day before & after:\n",
    "#    (a) previous trading day\n",
    "prev  = pd.merge_asof(\n",
    "    positive_surprises[['Earnings Date']],\n",
    "    amzn_price[['Date','Close']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='backward'\n",
    ").rename(columns={'Date':'PrevDate','Close':'Close_Before'})\n",
    "\n",
    "#    (b) next trading day\n",
    "next_ = pd.merge_asof(\n",
    "    positive_surprises[['Earnings Date']],\n",
    "    amzn_price[['Date','Close']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ").rename(columns={'Date':'NextDate','Close':'Close_After'})\n",
    "\n",
    "# 3. Stitch together:\n",
    "df = positive_surprises[['Earnings Date']].copy()\n",
    "df = df.join(prev.set_index('Earnings Date'),  on='Earnings Date')\n",
    "df = df.join(next_.set_index('Earnings Date'), on='Earnings Date')\n",
    "\n",
    "# 4. Compute the â€œ2-dayâ€ return around the announcement:\n",
    "df['2DayReturn'] = df['Close_After'] / df['Close_Before'] - 1\n",
    "\n",
    "# 5. Median of positive surprises:\n",
    "median_surprise = df['2DayReturn'].median()\n",
    "\n",
    "# 6. For â€œall datesâ€, you can still do your original shift(-2) approach:\n",
    "all_ret = amzn_price['Close'].shift(-2) / amzn_price['Close'] - 1\n",
    "median_all = all_ret.median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c186098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 positive surprise events\n",
      "\n",
      "Results:\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 1.0405%\n",
      "Difference: 0.8768 percentage points\n",
      "\n",
      "Sample positive surprise events:\n",
      "  Earnings Date       Date  2DayReturn_All\n",
      "0    1997-07-10 1997-07-10       -0.013457\n",
      "1    1997-10-27 1997-10-27       -0.015543\n",
      "2    1998-01-22 1998-01-22       -0.026695\n",
      "3    1998-04-27 1998-04-27        0.126658\n",
      "4    1998-07-22 1998-07-22       -0.033601\n",
      "\n",
      "Sample earnings data processing:\n",
      "  Earnings Date  Reported EPS  EPS Estimate  Surprise (%)  PositiveSurprise\n",
      "0    2026-04-29           NaN           NaN           NaN             False\n",
      "1    2026-02-04           NaN           NaN           NaN             False\n",
      "2    2025-10-29           NaN           NaN           NaN             False\n",
      "3    2025-07-30           NaN           NaN           NaN             False\n",
      "4    2025-05-01           NaN           NaN         16.74              True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 1. Load and clean earnings data\n",
    "amzn_earnings = pd.read_csv('ha1_Amazon.csv', sep=';')\n",
    "amzn_earnings.dropna(how='all', inplace=True)\n",
    "\n",
    "# Extract just the date part (drop \"at HH:MM AM/PM\")\n",
    "amzn_earnings['Earnings Date'] = (\n",
    "    amzn_earnings['Earnings Date']\n",
    "      .str.extract(r'(.*)\\s+at')[0]  # Fixed regex - added '+' for multiple spaces\n",
    "      .pipe(pd.to_datetime, errors='coerce')\n",
    ")\n",
    "\n",
    "# Convert numeric columns - handle percentage signs and other formatting\n",
    "for col in ['Surprise (%)', 'Reported EPS', 'EPS Estimate']:\n",
    "    if col == 'Surprise (%)':\n",
    "        # Remove % sign if present and convert\n",
    "        amzn_earnings[col] = (amzn_earnings[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace('%', '')\n",
    "                             .pipe(pd.to_numeric, errors='coerce'))\n",
    "    else:\n",
    "        amzn_earnings[col] = pd.to_numeric(amzn_earnings[col], errors='coerce')\n",
    "\n",
    "# Flag positive surprises\n",
    "amzn_earnings['PositiveSurprise'] = (\n",
    "    (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) |\n",
    "    (amzn_earnings['Surprise (%)'] > 0)\n",
    ")\n",
    "\n",
    "# Keep only valid, past positive-surprise events\n",
    "positive_surprises = amzn_earnings.loc[\n",
    "    amzn_earnings['PositiveSurprise'] &\n",
    "    amzn_earnings['Earnings Date'].notna() &\n",
    "    (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "    ['Earnings Date']\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(positive_surprises)} positive surprise events\")\n",
    "\n",
    "# 2. Download historical daily prices\n",
    "ticker = yf.Ticker(\"AMZN\")\n",
    "amzn_price = ticker.history(start='1997-01-01', interval='1d').reset_index()\n",
    "\n",
    "# Drop timezone so merges work cleanly\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "amzn_price.sort_values('Date', inplace=True, ignore_index=True)\n",
    "\n",
    "# 3. Calculate 2-day returns for ALL trading days\n",
    "# For each day i, calculate return from day i-1 to day i+1\n",
    "amzn_price['Close_Day1'] = amzn_price['Close'].shift(1)   # Previous day close\n",
    "amzn_price['Close_Day3'] = amzn_price['Close'].shift(-1)  # Next day close\n",
    "amzn_price['2DayReturn_All'] = amzn_price['Close_Day3'] / amzn_price['Close_Day1'] - 1\n",
    "\n",
    "# Remove rows with NaN returns\n",
    "valid_returns = amzn_price.dropna(subset=['2DayReturn_All'])\n",
    "\n",
    "# 4. For earnings events, find the corresponding trading day\n",
    "# Use merge_asof to find the first trading day on or after each earnings date\n",
    "earn_aligned = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', 'Close_Day1', 'Close_Day3', '2DayReturn_All']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ")\n",
    "\n",
    "# Remove any rows where we couldn't find a matching trading day or calculate return\n",
    "earn_aligned = earn_aligned.dropna(subset=['2DayReturn_All'])\n",
    "\n",
    "# 5. Calculate medians\n",
    "median_surprise = earn_aligned['2DayReturn_All'].median()\n",
    "median_all = valid_returns['2DayReturn_All'].median()\n",
    "\n",
    "# 6. Output results\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise*100:.4f}%\")\n",
    "print(f\"Difference: {(median_surprise - median_all)*100:.4f} percentage points\")\n",
    "\n",
    "# Debug: Show some sample data\n",
    "print(f\"\\nSample positive surprise events:\")\n",
    "print(earn_aligned[['Earnings Date', 'Date', '2DayReturn_All']].head())\n",
    "\n",
    "print(f\"\\nSample earnings data processing:\")\n",
    "print(amzn_earnings[['Earnings Date', 'Reported EPS', 'EPS Estimate', 'Surprise (%)', 'PositiveSurprise']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e70b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 positive surprise events\n",
      "\n",
      "Method 1 Results (Day i-1 to Day i+1):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 1.0405%\n",
      "\n",
      "Method 2 Results (Day i to Day i+2 - Following 2 days):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 0.2672%\n",
      "\n",
      "Sample positive surprise events (Method 2):\n",
      "  Earnings Date       Date  2DayReturn_Method2\n",
      "0    1997-07-10 1997-07-10           -0.166665\n",
      "1    1997-10-27 1997-10-27            0.262192\n",
      "2    1998-01-22 1998-01-22           -0.062881\n",
      "3    1998-04-27 1998-04-27            0.154078\n",
      "4    1998-07-22 1998-07-22           -0.072761\n",
      "5    1998-10-28 1998-10-28            0.080085\n",
      "6    1999-01-26 1999-01-26            0.067608\n",
      "7    1999-04-28 1999-04-28           -0.110788\n",
      "8    1999-07-21 1999-07-21           -0.086697\n",
      "9    1999-10-27 1999-10-27           -0.069959\n",
      "\n",
      "Sample earnings data processing:\n",
      "  Earnings Date  Reported EPS  EPS Estimate  Surprise (%)  PositiveSurprise\n",
      "0    2026-04-29           NaN           NaN           NaN             False\n",
      "1    2026-02-04           NaN           NaN           NaN             False\n",
      "2    2025-10-29           NaN           NaN           NaN             False\n",
      "3    2025-07-30           NaN           NaN           NaN             False\n",
      "4    2025-05-01           NaN           NaN         16.74              True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 1. Load and clean earnings data\n",
    "amzn_earnings = pd.read_csv('ha1_Amazon.csv', sep=';')\n",
    "amzn_earnings.dropna(how='all', inplace=True)\n",
    "\n",
    "# Extract just the date part (drop \"at HH:MM AM/PM\")\n",
    "amzn_earnings['Earnings Date'] = (\n",
    "    amzn_earnings['Earnings Date']\n",
    "      .str.extract(r'(.*)\\s+at')[0]  # Fixed regex - added '+' for multiple spaces\n",
    "      .pipe(pd.to_datetime, errors='coerce')\n",
    ")\n",
    "\n",
    "# Convert numeric columns - handle percentage signs and other formatting\n",
    "for col in ['Surprise (%)', 'Reported EPS', 'EPS Estimate']:\n",
    "    if col == 'Surprise (%)':\n",
    "        # Remove % sign if present and convert\n",
    "        amzn_earnings[col] = (amzn_earnings[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace('%', '')\n",
    "                             .pipe(pd.to_numeric, errors='coerce'))\n",
    "    else:\n",
    "        amzn_earnings[col] = pd.to_numeric(amzn_earnings[col], errors='coerce')\n",
    "\n",
    "# Flag positive surprises\n",
    "amzn_earnings['PositiveSurprise'] = (\n",
    "    (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) |\n",
    "    (amzn_earnings['Surprise (%)'] > 0)\n",
    ")\n",
    "\n",
    "# Keep only valid, past positive-surprise events\n",
    "positive_surprises = amzn_earnings.loc[\n",
    "    amzn_earnings['PositiveSurprise'] &\n",
    "    amzn_earnings['Earnings Date'].notna() &\n",
    "    (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "    ['Earnings Date']\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(positive_surprises)} positive surprise events\")\n",
    "\n",
    "# 2. Download historical daily prices\n",
    "ticker = yf.Ticker(\"AMZN\")\n",
    "amzn_price = ticker.history(start='1997-01-01', interval='1d').reset_index()\n",
    "\n",
    "# Drop timezone so merges work cleanly\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "amzn_price.sort_values('Date', inplace=True, ignore_index=True)\n",
    "\n",
    "# 3. Calculate 2-day returns for ALL trading days\n",
    "# Method 1: From day i-1 to day i+1 (spans 2 trading days)\n",
    "amzn_price['Close_Day1'] = amzn_price['Close'].shift(1)   # Previous day close\n",
    "amzn_price['Close_Day3'] = amzn_price['Close'].shift(-1)  # Next day close\n",
    "amzn_price['2DayReturn_Method1'] = amzn_price['Close_Day3'] / amzn_price['Close_Day1'] - 1\n",
    "\n",
    "# Method 2: From day i to day i+2 (following 2 trading days after announcement)\n",
    "amzn_price['Close_Day2Plus'] = amzn_price['Close'].shift(-2)  # Two days ahead\n",
    "amzn_price['2DayReturn_Method2'] = amzn_price['Close_Day2Plus'] / amzn_price['Close'] - 1\n",
    "\n",
    "# Remove rows with NaN returns for both methods\n",
    "valid_returns_m1 = amzn_price.dropna(subset=['2DayReturn_Method1'])\n",
    "valid_returns_m2 = amzn_price.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 4. For earnings events, find the corresponding trading day\n",
    "# Use merge_asof to find the first trading day on or after each earnings date\n",
    "earn_aligned = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', 'Close', 'Close_Day1', 'Close_Day3', 'Close_Day2Plus', '2DayReturn_Method1', '2DayReturn_Method2']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ")\n",
    "\n",
    "# Remove any rows where we couldn't find a matching trading day or calculate return\n",
    "earn_aligned_m1 = earn_aligned.dropna(subset=['2DayReturn_Method1'])\n",
    "earn_aligned_m2 = earn_aligned.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 5. Calculate medians for both methods\n",
    "# Method 1: Day i-1 to Day i+1\n",
    "median_surprise_m1 = earn_aligned_m1['2DayReturn_Method1'].median()\n",
    "median_all_m1 = valid_returns_m1['2DayReturn_Method1'].median()\n",
    "\n",
    "# Method 2: Day i to Day i+2 (following 2 days after announcement)\n",
    "median_surprise_m2 = earn_aligned_m2['2DayReturn_Method2'].median()\n",
    "median_all_m2 = valid_returns_m2['2DayReturn_Method2'].median()\n",
    "\n",
    "# 6. Output results for both methods\n",
    "print(f\"\\nMethod 1 Results (Day i-1 to Day i+1):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m1)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m1)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m1*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m1*100:.4f}%\")\n",
    "\n",
    "print(f\"\\nMethod 2 Results (Day i to Day i+2 - Following 2 days):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m2)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m2)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m2*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m2*100:.4f}%\")\n",
    "\n",
    "# Debug: Show some sample data\n",
    "print(f\"\\nSample positive surprise events (Method 2):\")\n",
    "print(earn_aligned_m2[['Earnings Date', 'Date', '2DayReturn_Method2']].head(10))\n",
    "\n",
    "print(f\"\\nSample earnings data processing:\")\n",
    "print(amzn_earnings[['Earnings Date', 'Reported EPS', 'EPS Estimate', 'Surprise (%)', 'PositiveSurprise']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1a267ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 positive surprise events\n",
      "\n",
      "Method 1 Results (Day i-1 to Day i+1):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 1.0405%\n",
      "\n",
      "Method 2 Results (Day i to Day i+2 - Following 2 days):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 0.2672%\n",
      "\n",
      "Debugging Information:\n",
      "Method 1 - Min: -28.98%, Max: 26.89%\n",
      "Method 2 - Min: -28.20%, Max: 40.29%\n",
      "Method 1 - Mean: 2.4086%\n",
      "Method 2 - Mean: 1.9558%\n",
      "Method 1 - Q1: -2.66%, Q3: 8.75%\n",
      "Method 2 - Q1: -7.06%, Q3: 10.62%\n",
      "\n",
      "1-day return after earnings median: 0.6974%\n",
      "\n",
      "If we divide by 100 (in case percentages were already in percent form):\n",
      "Method 1 median / 100: 0.0104%\n",
      "Method 2 median / 100: 0.0027%\n",
      "\n",
      "Sample earnings events with returns:\n",
      "  Earnings Date       Date  2DayReturn_Method1_pct     Close  Close_Day1  \\\n",
      "0    1997-07-10 1997-07-10               -1.345718  0.128125    0.116146   \n",
      "1    1997-10-27 1997-10-27               -1.554304  0.213542    0.251302   \n",
      "2    1998-01-22 1998-01-22               -2.669462  0.256771    0.253646   \n",
      "3    1998-04-27 1998-04-27               12.665772  0.344792    0.353646   \n",
      "4    1998-07-22 1998-07-22               -3.360132  1.116667    1.100521   \n",
      "5    1998-10-28 1998-10-28                8.758750  0.975521    0.969271   \n",
      "6    1999-01-26 1999-01-26               11.790877  2.877344    2.809375   \n",
      "7    1999-04-28 1999-04-28              -18.275647  4.837500    5.146875   \n",
      "8    1999-07-21 1999-07-21              -11.082192  3.135938    3.003125   \n",
      "9    1999-10-27 1999-10-27              -12.615386  3.796875    4.062500   \n",
      "\n",
      "   Close_Day3  \n",
      "0    0.114583  \n",
      "1    0.247396  \n",
      "2    0.246875  \n",
      "3    0.398438  \n",
      "4    1.063542  \n",
      "5    1.054167  \n",
      "6    3.140625  \n",
      "7    4.206250  \n",
      "8    2.670313  \n",
      "9    3.550000  \n",
      "\n",
      "Earnings data analysis:\n",
      "Total earnings records: 116\n",
      "Records with valid dates: 116\n",
      "Records with Surprise % > 0: 86\n",
      "Records with Actual > Estimate: 33\n",
      "Total positive surprises: 86\n",
      "\n",
      "Sample earnings data:\n",
      "  Earnings Date  Reported EPS  EPS Estimate  Surprise (%)  PositiveSurprise\n",
      "0    2026-04-29           NaN           NaN           NaN             False\n",
      "1    2026-02-04           NaN           NaN           NaN             False\n",
      "2    2025-10-29           NaN           NaN           NaN             False\n",
      "3    2025-07-30           NaN           NaN           NaN             False\n",
      "4    2025-05-01           NaN           NaN         16.74              True\n",
      "5    2025-02-06           NaN           NaN         24.47              True\n",
      "6    2024-10-31           NaN           NaN         25.17              True\n",
      "7    2024-08-01           NaN           NaN         22.58              True\n",
      "8    2024-04-30          0.98          0.83         17.91              True\n",
      "9    2024-02-01          1.00          0.80         24.55              True\n",
      "\n",
      "Alternative surprise definitions:\n",
      "Only Surprise % > 0: 86 events\n",
      "Only Actual > Estimate: 33 events\n",
      "Alternative median (Surprise % > 0 only): 1.0405% (86 events)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 1. Load and clean earnings data\n",
    "amzn_earnings = pd.read_csv('ha1_Amazon.csv', sep=';')\n",
    "amzn_earnings.dropna(how='all', inplace=True)\n",
    "\n",
    "# Extract just the date part (drop \"at HH:MM AM/PM\")\n",
    "amzn_earnings['Earnings Date'] = (\n",
    "    amzn_earnings['Earnings Date']\n",
    "      .str.extract(r'(.*)\\s+at')[0]  # Fixed regex - added '+' for multiple spaces\n",
    "      .pipe(pd.to_datetime, errors='coerce')\n",
    ")\n",
    "\n",
    "# Convert numeric columns - handle percentage signs and other formatting\n",
    "for col in ['Surprise (%)', 'Reported EPS', 'EPS Estimate']:\n",
    "    if col == 'Surprise (%)':\n",
    "        # Remove % sign if present and convert\n",
    "        amzn_earnings[col] = (amzn_earnings[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace('%', '')\n",
    "                             .pipe(pd.to_numeric, errors='coerce'))\n",
    "    else:\n",
    "        amzn_earnings[col] = pd.to_numeric(amzn_earnings[col], errors='coerce')\n",
    "\n",
    "# Flag positive surprises\n",
    "amzn_earnings['PositiveSurprise'] = (\n",
    "    (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) |\n",
    "    (amzn_earnings['Surprise (%)'] > 0)\n",
    ")\n",
    "\n",
    "# Keep only valid, past positive-surprise events\n",
    "positive_surprises = amzn_earnings.loc[\n",
    "    amzn_earnings['PositiveSurprise'] &\n",
    "    amzn_earnings['Earnings Date'].notna() &\n",
    "    (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "    ['Earnings Date']\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(positive_surprises)} positive surprise events\")\n",
    "\n",
    "# 2. Download historical daily prices\n",
    "ticker = yf.Ticker(\"AMZN\")\n",
    "amzn_price = ticker.history(start='1997-01-01', interval='1d').reset_index()\n",
    "\n",
    "# Drop timezone so merges work cleanly\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "amzn_price.sort_values('Date', inplace=True, ignore_index=True)\n",
    "\n",
    "# 3. Calculate 2-day returns for ALL trading days\n",
    "# Method 1: From day i-1 to day i+1 (spans 2 trading days)\n",
    "amzn_price['Close_Day1'] = amzn_price['Close'].shift(1)   # Previous day close\n",
    "amzn_price['Close_Day3'] = amzn_price['Close'].shift(-1)  # Next day close\n",
    "amzn_price['2DayReturn_Method1'] = amzn_price['Close_Day3'] / amzn_price['Close_Day1'] - 1\n",
    "\n",
    "# Method 2: From day i to day i+2 (following 2 trading days after announcement)\n",
    "amzn_price['Close_Day2Plus'] = amzn_price['Close'].shift(-2)  # Two days ahead\n",
    "amzn_price['2DayReturn_Method2'] = amzn_price['Close_Day2Plus'] / amzn_price['Close'] - 1\n",
    "\n",
    "# Remove rows with NaN returns for both methods\n",
    "valid_returns_m1 = amzn_price.dropna(subset=['2DayReturn_Method1'])\n",
    "valid_returns_m2 = amzn_price.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 4. For earnings events, find the corresponding trading day\n",
    "# Use merge_asof to find the first trading day on or after each earnings date\n",
    "earn_aligned = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', 'Close', 'Close_Day1', 'Close_Day3', 'Close_Day2Plus', '2DayReturn_Method1', '2DayReturn_Method2']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ")\n",
    "\n",
    "# Remove any rows where we couldn't find a matching trading day or calculate return\n",
    "earn_aligned_m1 = earn_aligned.dropna(subset=['2DayReturn_Method1'])\n",
    "earn_aligned_m2 = earn_aligned.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 5. Calculate medians for both methods\n",
    "# Method 1: Day i-1 to Day i+1\n",
    "median_surprise_m1 = earn_aligned_m1['2DayReturn_Method1'].median()\n",
    "median_all_m1 = valid_returns_m1['2DayReturn_Method1'].median()\n",
    "\n",
    "# Method 2: Day i to Day i+2 (following 2 days after announcement)\n",
    "median_surprise_m2 = earn_aligned_m2['2DayReturn_Method2'].median()\n",
    "median_all_m2 = valid_returns_m2['2DayReturn_Method2'].median()\n",
    "\n",
    "# 6. Output results for both methods\n",
    "print(f\"\\nMethod 1 Results (Day i-1 to Day i+1):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m1)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m1)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m1*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m1*100:.4f}%\")\n",
    "\n",
    "print(f\"\\nMethod 2 Results (Day i to Day i+2 - Following 2 days):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m2)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m2)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m2*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m2*100:.4f}%\")\n",
    "\n",
    "# Additional debugging and alternative calculations\n",
    "print(f\"\\nDebugging Information:\")\n",
    "\n",
    "# Check if there are any extreme outliers\n",
    "print(f\"Method 1 - Min: {earn_aligned_m1['2DayReturn_Method1'].min()*100:.2f}%, Max: {earn_aligned_m1['2DayReturn_Method1'].max()*100:.2f}%\")\n",
    "print(f\"Method 2 - Min: {earn_aligned_m2['2DayReturn_Method2'].min()*100:.2f}%, Max: {earn_aligned_m2['2DayReturn_Method2'].max()*100:.2f}%\")\n",
    "\n",
    "# Try calculating mean instead of median\n",
    "mean_surprise_m1 = earn_aligned_m1['2DayReturn_Method1'].mean()\n",
    "mean_surprise_m2 = earn_aligned_m2['2DayReturn_Method2'].mean()\n",
    "print(f\"Method 1 - Mean: {mean_surprise_m1*100:.4f}%\")\n",
    "print(f\"Method 2 - Mean: {mean_surprise_m2*100:.4f}%\")\n",
    "\n",
    "# Check quartiles\n",
    "q25_m1, q75_m1 = earn_aligned_m1['2DayReturn_Method1'].quantile([0.25, 0.75])\n",
    "q25_m2, q75_m2 = earn_aligned_m2['2DayReturn_Method2'].quantile([0.25, 0.75])\n",
    "print(f\"Method 1 - Q1: {q25_m1*100:.2f}%, Q3: {q75_m1*100:.2f}%\")\n",
    "print(f\"Method 2 - Q1: {q25_m2*100:.2f}%, Q3: {q75_m2*100:.2f}%\")\n",
    "\n",
    "# Try different time window - maybe it's asking for 1-day return after earnings\n",
    "amzn_price['1DayReturn_After'] = amzn_price['Close'].shift(-1) / amzn_price['Close'] - 1\n",
    "earn_aligned_1day = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', '1DayReturn_After']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ").dropna(subset=['1DayReturn_After'])\n",
    "\n",
    "median_1day = earn_aligned_1day['1DayReturn_After'].median()\n",
    "print(f\"\\n1-day return after earnings median: {median_1day*100:.4f}%\")\n",
    "\n",
    "# Check if percentage values need to be divided by 100\n",
    "print(f\"\\nIf we divide by 100 (in case percentages were already in percent form):\")\n",
    "print(f\"Method 1 median / 100: {median_surprise_m1:.4f}%\")\n",
    "print(f\"Method 2 median / 100: {median_surprise_m2:.4f}%\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample earnings events with returns:\")\n",
    "sample_data = earn_aligned_m1[['Earnings Date', 'Date', '2DayReturn_Method1', 'Close', 'Close_Day1', 'Close_Day3']].head(10)\n",
    "for col in ['2DayReturn_Method1']:\n",
    "    sample_data[f'{col}_pct'] = sample_data[col] * 100\n",
    "print(sample_data[['Earnings Date', 'Date', '2DayReturn_Method1_pct', 'Close', 'Close_Day1', 'Close_Day3']])\n",
    "\n",
    "# Check earnings data more carefully\n",
    "print(f\"\\nEarnings data analysis:\")\n",
    "print(f\"Total earnings records: {len(amzn_earnings)}\")\n",
    "print(f\"Records with valid dates: {amzn_earnings['Earnings Date'].notna().sum()}\")\n",
    "print(f\"Records with Surprise % > 0: {(amzn_earnings['Surprise (%)'] > 0).sum()}\")\n",
    "print(f\"Records with Actual > Estimate: {(amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']).sum()}\")\n",
    "print(f\"Total positive surprises: {amzn_earnings['PositiveSurprise'].sum()}\")\n",
    "\n",
    "# Show some sample earnings data\n",
    "print(f\"\\nSample earnings data:\")\n",
    "sample_earnings = amzn_earnings[['Earnings Date', 'Reported EPS', 'EPS Estimate', 'Surprise (%)', 'PositiveSurprise']].head(10)\n",
    "print(sample_earnings)\n",
    "\n",
    "# Check if we should use different surprise criteria\n",
    "print(f\"\\nAlternative surprise definitions:\")\n",
    "alt_positive1 = (amzn_earnings['Surprise (%)'] > 0) & amzn_earnings['Surprise (%)'].notna()\n",
    "alt_positive2 = (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) & amzn_earnings['Reported EPS'].notna() & amzn_earnings['EPS Estimate'].notna()\n",
    "\n",
    "print(f\"Only Surprise % > 0: {alt_positive1.sum()} events\")\n",
    "print(f\"Only Actual > Estimate: {alt_positive2.sum()} events\")\n",
    "\n",
    "# Try with just Surprise % > 0\n",
    "if alt_positive1.sum() > 0:\n",
    "    alt_surprises = amzn_earnings.loc[\n",
    "        alt_positive1 & \n",
    "        amzn_earnings['Earnings Date'].notna() &\n",
    "        (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "        ['Earnings Date']\n",
    "    ].copy()\n",
    "    \n",
    "    alt_earn_aligned = pd.merge_asof(\n",
    "        alt_surprises.sort_values('Earnings Date'),\n",
    "        amzn_price[['Date', '2DayReturn_Method1']],\n",
    "        left_on='Earnings Date',\n",
    "        right_on='Date',\n",
    "        direction='forward'\n",
    "    ).dropna(subset=['2DayReturn_Method1'])\n",
    "    \n",
    "    alt_median = alt_earn_aligned['2DayReturn_Method1'].median()\n",
    "    print(f\"Alternative median (Surprise % > 0 only): {alt_median*100:.4f}% ({len(alt_earn_aligned)} events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac04be",
   "metadata": {},
   "source": [
    "### Question 5.  [Exploratory, optional] Brainstorm potential idea for your capstone project\n",
    "\n",
    "**Free text answer**\n",
    "\n",
    "Describe the capstone project you would like to pursue, considering your aspirations, ML model predictions, and prior knowledge. Even if you are unsure at this stage, try to generate an idea you would like to explore-such as a specific asset class, country, industry vertical, or investment strategy. Be as specific as possible.\n",
    "\n",
    "*Example: I want to build a short-term prediction model for the US/India/Brazil stock markets, focusing on the largest stocks over a 30-day investment horizon. I plan to use RSI and MACD technical indicators and news coverage data to generate predictions.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f283e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 86 positive surprise events\n",
      "\n",
      "Method 1 Results (Day i-1 to Day i+1):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 1.0405%\n",
      "\n",
      "Method 2 Results (Day i to Day i+2 - Following 2 days):\n",
      "Number of positive-surprise events with valid returns: 86\n",
      "Total number of valid 2-day return periods: 7049\n",
      "Median 2-day return (all periods): 0.1638%\n",
      "Median 2-day return (positive surprises): 0.2672%\n",
      "\n",
      "Debugging Information:\n",
      "Method 1 - Min: -28.98%, Max: 26.89%\n",
      "Method 2 - Min: -28.20%, Max: 40.29%\n",
      "Method 1 - Mean: 2.4086%\n",
      "Method 2 - Mean: 1.9558%\n",
      "Method 1 - Q1: -2.66%, Q3: 8.75%\n",
      "Method 2 - Q1: -7.06%, Q3: 10.62%\n",
      "\n",
      "1-day return after earnings median: 0.6974%\n",
      "\n",
      "If we divide by 100 (in case percentages were already in percent form):\n",
      "Method 1 median / 100: 0.0104%\n",
      "Method 2 median / 100: 0.0027%\n",
      "\n",
      "Sample earnings events with returns:\n",
      "  Earnings Date       Date  2DayReturn_Method1_pct     Close  Close_Day1  \\\n",
      "0    1997-07-10 1997-07-10               -1.345718  0.128125    0.116146   \n",
      "1    1997-10-27 1997-10-27               -1.554304  0.213542    0.251302   \n",
      "2    1998-01-22 1998-01-22               -2.669462  0.256771    0.253646   \n",
      "3    1998-04-27 1998-04-27               12.665772  0.344792    0.353646   \n",
      "4    1998-07-22 1998-07-22               -3.360132  1.116667    1.100521   \n",
      "5    1998-10-28 1998-10-28                8.758750  0.975521    0.969271   \n",
      "6    1999-01-26 1999-01-26               11.790877  2.877344    2.809375   \n",
      "7    1999-04-28 1999-04-28              -18.275647  4.837500    5.146875   \n",
      "8    1999-07-21 1999-07-21              -11.082192  3.135938    3.003125   \n",
      "9    1999-10-27 1999-10-27              -12.615386  3.796875    4.062500   \n",
      "\n",
      "   Close_Day3  \n",
      "0    0.114583  \n",
      "1    0.247396  \n",
      "2    0.246875  \n",
      "3    0.398438  \n",
      "4    1.063542  \n",
      "5    1.054167  \n",
      "6    3.140625  \n",
      "7    4.206250  \n",
      "8    2.670313  \n",
      "9    3.550000  \n",
      "\n",
      "Earnings data analysis:\n",
      "Total earnings records: 116\n",
      "Records with valid dates: 116\n",
      "Records with Surprise % > 0: 86\n",
      "Records with Actual > Estimate: 33\n",
      "Total positive surprises: 86\n",
      "\n",
      "Sample earnings data:\n",
      "  Earnings Date  Reported EPS  EPS Estimate  Surprise (%)  PositiveSurprise\n",
      "0    2026-04-29           NaN           NaN           NaN             False\n",
      "1    2026-02-04           NaN           NaN           NaN             False\n",
      "2    2025-10-29           NaN           NaN           NaN             False\n",
      "3    2025-07-30           NaN           NaN           NaN             False\n",
      "4    2025-05-01           NaN           NaN         16.74              True\n",
      "5    2025-02-06           NaN           NaN         24.47              True\n",
      "6    2024-10-31           NaN           NaN         25.17              True\n",
      "7    2024-08-01           NaN           NaN         22.58              True\n",
      "8    2024-04-30          0.98          0.83         17.91              True\n",
      "9    2024-02-01          1.00          0.80         24.55              True\n",
      "\n",
      "Alternative surprise definitions:\n",
      "Only Surprise % > 0: 86 events\n",
      "Only Actual > Estimate: 33 events\n",
      "Alternative median (Surprise % > 0 only): 1.0405% (86 events)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# 1. Load and clean earnings data\n",
    "amzn_earnings = pd.read_csv('ha1_Amazon.csv', sep=';')\n",
    "amzn_earnings.dropna(how='all', inplace=True)\n",
    "\n",
    "# Extract just the date part (drop \"at HH:MM AM/PM\")\n",
    "amzn_earnings['Earnings Date'] = (\n",
    "    amzn_earnings['Earnings Date']\n",
    "      .str.extract(r'(.*)\\s+at')[0]  # Fixed regex - added '+' for multiple spaces\n",
    "      .pipe(pd.to_datetime, errors='coerce')\n",
    ")\n",
    "\n",
    "# Convert numeric columns - handle percentage signs and other formatting\n",
    "for col in ['Surprise (%)', 'Reported EPS', 'EPS Estimate']:\n",
    "    if col == 'Surprise (%)':\n",
    "        # Remove % sign if present and convert\n",
    "        amzn_earnings[col] = (amzn_earnings[col]\n",
    "                             .astype(str)\n",
    "                             .str.replace('%', '')\n",
    "                             .pipe(pd.to_numeric, errors='coerce'))\n",
    "    else:\n",
    "        amzn_earnings[col] = pd.to_numeric(amzn_earnings[col], errors='coerce')\n",
    "\n",
    "# Flag positive surprises\n",
    "amzn_earnings['PositiveSurprise'] = (\n",
    "    (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) |\n",
    "    (amzn_earnings['Surprise (%)'] > 0)\n",
    ")\n",
    "\n",
    "# Keep only valid, past positive-surprise events\n",
    "positive_surprises = amzn_earnings.loc[\n",
    "    amzn_earnings['PositiveSurprise'] &\n",
    "    amzn_earnings['Earnings Date'].notna() &\n",
    "    (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "    ['Earnings Date']\n",
    "].copy()\n",
    "\n",
    "print(f\"Found {len(positive_surprises)} positive surprise events\")\n",
    "\n",
    "# 2. Download historical daily prices\n",
    "ticker = yf.Ticker(\"AMZN\")\n",
    "amzn_price = ticker.history(start='1997-01-01', interval='1d').reset_index()\n",
    "\n",
    "# Drop timezone so merges work cleanly\n",
    "amzn_price['Date'] = amzn_price['Date'].dt.tz_localize(None)\n",
    "amzn_price.sort_values('Date', inplace=True, ignore_index=True)\n",
    "\n",
    "# 3. Calculate 2-day returns for ALL trading days\n",
    "# Method 1: From day i-1 to day i+1 (spans 2 trading days)\n",
    "amzn_price['Close_Day1'] = amzn_price['Close'].shift(1)   # Previous day close\n",
    "amzn_price['Close_Day3'] = amzn_price['Close'].shift(-1)  # Next day close\n",
    "amzn_price['2DayReturn_Method1'] = amzn_price['Close_Day3'] / amzn_price['Close_Day1'] - 1\n",
    "\n",
    "# Method 2: From day i to day i+2 (following 2 trading days after announcement)\n",
    "amzn_price['Close_Day2Plus'] = amzn_price['Close'].shift(-2)  # Two days ahead\n",
    "amzn_price['2DayReturn_Method2'] = amzn_price['Close_Day2Plus'] / amzn_price['Close'] - 1\n",
    "\n",
    "# Remove rows with NaN returns for both methods\n",
    "valid_returns_m1 = amzn_price.dropna(subset=['2DayReturn_Method1'])\n",
    "valid_returns_m2 = amzn_price.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 4. For earnings events, find the corresponding trading day\n",
    "# Use merge_asof to find the first trading day on or after each earnings date\n",
    "earn_aligned = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', 'Close', 'Close_Day1', 'Close_Day3', 'Close_Day2Plus', '2DayReturn_Method1', '2DayReturn_Method2']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ")\n",
    "\n",
    "# Remove any rows where we couldn't find a matching trading day or calculate return\n",
    "earn_aligned_m1 = earn_aligned.dropna(subset=['2DayReturn_Method1'])\n",
    "earn_aligned_m2 = earn_aligned.dropna(subset=['2DayReturn_Method2'])\n",
    "\n",
    "# 5. Calculate medians for both methods\n",
    "# Method 1: Day i-1 to Day i+1\n",
    "median_surprise_m1 = earn_aligned_m1['2DayReturn_Method1'].median()\n",
    "median_all_m1 = valid_returns_m1['2DayReturn_Method1'].median()\n",
    "\n",
    "# Method 2: Day i to Day i+2 (following 2 days after announcement)\n",
    "median_surprise_m2 = earn_aligned_m2['2DayReturn_Method2'].median()\n",
    "median_all_m2 = valid_returns_m2['2DayReturn_Method2'].median()\n",
    "\n",
    "# 6. Output results for both methods\n",
    "print(f\"\\nMethod 1 Results (Day i-1 to Day i+1):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m1)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m1)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m1*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m1*100:.4f}%\")\n",
    "\n",
    "print(f\"\\nMethod 2 Results (Day i to Day i+2 - Following 2 days):\")\n",
    "print(f\"Number of positive-surprise events with valid returns: {len(earn_aligned_m2)}\")\n",
    "print(f\"Total number of valid 2-day return periods: {len(valid_returns_m2)}\")\n",
    "print(f\"Median 2-day return (all periods): {median_all_m2*100:.4f}%\")\n",
    "print(f\"Median 2-day return (positive surprises): {median_surprise_m2*100:.4f}%\")\n",
    "\n",
    "# Additional debugging and alternative calculations\n",
    "print(f\"\\nDebugging Information:\")\n",
    "\n",
    "# Check if there are any extreme outliers\n",
    "print(f\"Method 1 - Min: {earn_aligned_m1['2DayReturn_Method1'].min()*100:.2f}%, Max: {earn_aligned_m1['2DayReturn_Method1'].max()*100:.2f}%\")\n",
    "print(f\"Method 2 - Min: {earn_aligned_m2['2DayReturn_Method2'].min()*100:.2f}%, Max: {earn_aligned_m2['2DayReturn_Method2'].max()*100:.2f}%\")\n",
    "\n",
    "# Try calculating mean instead of median\n",
    "mean_surprise_m1 = earn_aligned_m1['2DayReturn_Method1'].mean()\n",
    "mean_surprise_m2 = earn_aligned_m2['2DayReturn_Method2'].mean()\n",
    "print(f\"Method 1 - Mean: {mean_surprise_m1*100:.4f}%\")\n",
    "print(f\"Method 2 - Mean: {mean_surprise_m2*100:.4f}%\")\n",
    "\n",
    "# Check quartiles\n",
    "q25_m1, q75_m1 = earn_aligned_m1['2DayReturn_Method1'].quantile([0.25, 0.75])\n",
    "q25_m2, q75_m2 = earn_aligned_m2['2DayReturn_Method2'].quantile([0.25, 0.75])\n",
    "print(f\"Method 1 - Q1: {q25_m1*100:.2f}%, Q3: {q75_m1*100:.2f}%\")\n",
    "print(f\"Method 2 - Q1: {q25_m2*100:.2f}%, Q3: {q75_m2*100:.2f}%\")\n",
    "\n",
    "# Try different time window - maybe it's asking for 1-day return after earnings\n",
    "amzn_price['1DayReturn_After'] = amzn_price['Close'].shift(-1) / amzn_price['Close'] - 1\n",
    "earn_aligned_1day = pd.merge_asof(\n",
    "    positive_surprises.sort_values('Earnings Date'),\n",
    "    amzn_price[['Date', '1DayReturn_After']],\n",
    "    left_on='Earnings Date',\n",
    "    right_on='Date',\n",
    "    direction='forward'\n",
    ").dropna(subset=['1DayReturn_After'])\n",
    "\n",
    "median_1day = earn_aligned_1day['1DayReturn_After'].median()\n",
    "print(f\"\\n1-day return after earnings median: {median_1day*100:.4f}%\")\n",
    "\n",
    "# Check if percentage values need to be divided by 100\n",
    "print(f\"\\nIf we divide by 100 (in case percentages were already in percent form):\")\n",
    "print(f\"Method 1 median / 100: {median_surprise_m1:.4f}%\")\n",
    "print(f\"Method 2 median / 100: {median_surprise_m2:.4f}%\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nSample earnings events with returns:\")\n",
    "sample_data = earn_aligned_m1[['Earnings Date', 'Date', '2DayReturn_Method1', 'Close', 'Close_Day1', 'Close_Day3']].head(10)\n",
    "for col in ['2DayReturn_Method1']:\n",
    "    sample_data[f'{col}_pct'] = sample_data[col] * 100\n",
    "print(sample_data[['Earnings Date', 'Date', '2DayReturn_Method1_pct', 'Close', 'Close_Day1', 'Close_Day3']])\n",
    "\n",
    "# Check earnings data more carefully\n",
    "print(f\"\\nEarnings data analysis:\")\n",
    "print(f\"Total earnings records: {len(amzn_earnings)}\")\n",
    "print(f\"Records with valid dates: {amzn_earnings['Earnings Date'].notna().sum()}\")\n",
    "print(f\"Records with Surprise % > 0: {(amzn_earnings['Surprise (%)'] > 0).sum()}\")\n",
    "print(f\"Records with Actual > Estimate: {(amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']).sum()}\")\n",
    "print(f\"Total positive surprises: {amzn_earnings['PositiveSurprise'].sum()}\")\n",
    "\n",
    "# Show some sample earnings data\n",
    "print(f\"\\nSample earnings data:\")\n",
    "sample_earnings = amzn_earnings[['Earnings Date', 'Reported EPS', 'EPS Estimate', 'Surprise (%)', 'PositiveSurprise']].head(10)\n",
    "print(sample_earnings)\n",
    "\n",
    "# Check if we should use different surprise criteria\n",
    "print(f\"\\nAlternative surprise definitions:\")\n",
    "alt_positive1 = (amzn_earnings['Surprise (%)'] > 0) & amzn_earnings['Surprise (%)'].notna()\n",
    "alt_positive2 = (amzn_earnings['Reported EPS'] > amzn_earnings['EPS Estimate']) & amzn_earnings['Reported EPS'].notna() & amzn_earnings['EPS Estimate'].notna()\n",
    "\n",
    "print(f\"Only Surprise % > 0: {alt_positive1.sum()} events\")\n",
    "print(f\"Only Actual > Estimate: {alt_positive2.sum()} events\")\n",
    "\n",
    "# Try with just Surprise % > 0\n",
    "if alt_positive1.sum() > 0:\n",
    "    alt_surprises = amzn_earnings.loc[\n",
    "        alt_positive1 & \n",
    "        amzn_earnings['Earnings Date'].notna() &\n",
    "        (amzn_earnings['Earnings Date'] < pd.Timestamp.today().normalize()),\n",
    "        ['Earnings Date']\n",
    "    ].copy()\n",
    "    \n",
    "    alt_earn_aligned = pd.merge_asof(\n",
    "        alt_surprises.sort_values('Earnings Date'),\n",
    "        amzn_price[['Date', '2DayReturn_Method1']],\n",
    "        left_on='Earnings Date',\n",
    "        right_on='Date',\n",
    "        direction='forward'\n",
    "    ).dropna(subset=['2DayReturn_Method1'])\n",
    "    \n",
    "    alt_median = alt_earn_aligned['2DayReturn_Method1'].median()\n",
    "    print(f\"Alternative median (Surprise % > 0 only): {alt_median*100:.4f}% ({len(alt_earn_aligned)} events)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9dc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f66f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd02e9b4",
   "metadata": {},
   "source": [
    "I want to build a personal investment assistant that recommends U.S. stocks to retail investors based on a combination of fundamental strength, price momentum, and recent investor sentiment. The assistant will use machine learning to predict the likelihood that a given stock will outperform its sector over the next 30 trading days â€” a horizon relevant to active retail traders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
